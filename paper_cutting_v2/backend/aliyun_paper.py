#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é˜¿é‡Œäº‘ç²¾ç»†ç‰ˆç»“æ„åŒ–åˆ‡é¢˜è„šæœ¬
ç”¨äºè¯†åˆ«è¯•å·å›¾ç‰‡ä¸­çš„è¯•é¢˜,åŒ…æ‹¬é¢˜ç›®å†…å®¹ã€é€‰é¡¹ã€ä½œç­”åŒºåŸŸå’Œå›¾ç‰‡
"""

import json
import os
import time
from pathlib import Path
from typing import Dict, List, Optional
from alibabacloud_ocr_api20210707.client import Client as OcrClient
from alibabacloud_tea_openapi import models as open_api_models
from alibabacloud_ocr_api20210707 import models as ocr_models  # ty:ignore[unresolved-import]
from alibabacloud_tea_util import models as util_models
import io


class AliyunPaperOCR:
    """é˜¿é‡Œäº‘è¯•å·OCRè¯†åˆ«ç±»"""

    def __init__(self, access_key_id: str, access_key_secret: str):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯

        Args:
            access_key_id: é˜¿é‡Œäº‘AccessKey ID
            access_key_secret: é˜¿é‡Œäº‘AccessKey Secret
        """
        config = open_api_models.Config(
            access_key_id=access_key_id, access_key_secret=access_key_secret
        )
        config.endpoint = "ocr-api.cn-hangzhou.aliyuncs.com"
        self.client = OcrClient(config)

    def recognize_paper(
        self,
        image_path: str = None,
        image_url: str = None,
        subject: str = "default",
        need_rotate: bool = True,
        output_origin_points: bool = True,
        extract_images: bool = False,
        output_dir: str = None,
        min_figure_area: int = 2000,
        extract_figures: bool = True,
        extract_answer_areas: bool = False,
    ) -> Dict:
        """
        è¯†åˆ«è¯•å·å›¾ç‰‡

        Args:
            image_path: æœ¬åœ°å›¾ç‰‡è·¯å¾„
            image_url: å›¾ç‰‡URLåœ°å€
            subject: å¹´çº§å­¦ç§‘(default, Math, Chinese, Englishç­‰)
            need_rotate: æ˜¯å¦éœ€è¦è‡ªåŠ¨æ—‹è½¬
            output_origin_points: æ˜¯å¦è¾“å‡ºåŸå›¾åæ ‡
            extract_images: æ˜¯å¦æå–é¢˜ç›®ä¸­çš„å›¾ç‰‡
            output_dir: å›¾ç‰‡è¾“å‡ºç›®å½•
            min_figure_area: å›¾ç‰‡æœ€å°é¢ç§¯é˜ˆå€¼
            extract_figures: æ˜¯å¦åˆ‡å›¾æå–é¢˜ç›®é…å›¾
            extract_answer_areas: æ˜¯å¦åˆ‡å›¾æå–ä½œç­”åŒºåŸŸ

        Returns:
            è¯†åˆ«ç»“æœå­—å…¸
        """
        start_time = time.time()

        request = ocr_models.RecognizeEduPaperStructedRequest()

        # è®¾ç½®å›¾ç‰‡æ¥æº(URLæˆ–æœ¬åœ°æ–‡ä»¶äºŒé€‰ä¸€)
        if image_url:
            request.url = image_url
        elif image_path:
            with open(image_path, "rb") as f:
                request.body = io.BytesIO(f.read())
        else:
            raise ValueError("å¿…é¡»æä¾› image_path æˆ– image_url ä¹‹ä¸€")

        # è®¾ç½®è¯†åˆ«å‚æ•°
        request.type = subject
        request.need_rotate = need_rotate
        request.output_origin_points = output_origin_points

        runtime = util_models.RuntimeOptions()

        try:
            response = self.client.recognize_edu_paper_structed_with_options(
                request, runtime
            )
            result = response.body.to_map()

            elapsed_time = time.time() - start_time
            print(f"â±ï¸  OCRè¯†åˆ«è€—æ—¶: {elapsed_time:.2f}ç§’")

            # å¦‚æœéœ€è¦æå–å›¾ç‰‡
            if extract_images and image_path and output_dir:
                questions = self.parse_questions(result, page_index=1)
                self._extract_question_images(
                    questions,
                    image_path,
                    output_dir,
                    min_figure_area=min_figure_area,
                    extract_figures=extract_figures,
                    extract_answer_areas=extract_answer_areas,
                )

            return result
        except Exception as e:
            print(f"è¯†åˆ«å¤±è´¥: {str(e)}")
            raise

    def recognize_exam_directory(
        self,
        exam_dir: str,
        subject: str = "default",
        need_rotate: bool = True,
        output_origin_points: bool = True,
        extract_images: bool = True,
        output_dir: str = "extracted_images",
        save_raw_ocr: Optional[str] = None,  # ä¿å­˜åŸå§‹OCRç»“æœçš„æ–‡ä»¶è·¯å¾„
        save_processed: Optional[str] = None,  # ä¿å­˜å¤„ç†åç»“æœçš„æ–‡ä»¶è·¯å¾„
        min_figure_area: int = 2000,
        extract_figures: bool = True,
        extract_answer_areas: bool = False,
    ) -> Dict:
        """
        è¯†åˆ«æ•´ä¸ªè¯•å·ç›®å½•

        Args:
            exam_dir: è¯•å·å›¾ç‰‡ç›®å½•
            subject: å¹´çº§å­¦ç§‘
            need_rotate: æ˜¯å¦éœ€è¦è‡ªåŠ¨æ—‹è½¬
            output_origin_points: æ˜¯å¦è¾“å‡ºåŸå›¾åæ ‡
            extract_images: æ˜¯å¦æå–é¢˜ç›®ä¸­çš„å›¾ç‰‡
            output_dir: å›¾ç‰‡è¾“å‡ºç›®å½•
            save_raw_ocr: ä¿å­˜åŸå§‹OCRç»“æœçš„JSONæ–‡ä»¶è·¯å¾„(å¦‚aly.json)
            save_processed: ä¿å­˜å¤„ç†åç»“æœçš„JSONæ–‡ä»¶è·¯å¾„(å¦‚out.json)
            min_figure_area: å›¾ç‰‡æœ€å°é¢ç§¯é˜ˆå€¼
            extract_figures: æ˜¯å¦åˆ‡å›¾æå–é¢˜ç›®é…å›¾
            extract_answer_areas: æ˜¯å¦åˆ‡å›¾æå–ä½œç­”åŒºåŸŸ

        Returns:
            ä¼˜åŒ–åçš„ç»“æ„åŒ–æ•°æ®
        """
        total_start_time = time.time()

        image_files = self._list_image_files(exam_dir)
        if not image_files:
            raise ValueError(f"ç›®å½•ä¸­æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶: {exam_dir}")

        print(f"ğŸ“ æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡")
        
        # ä½¿ç”¨partsç»“æ„æ¥ç»„ç»‡æ‰€æœ‰é¢˜ç›®
        all_parts = []
        total_questions_count = 0
        extracted_images_count = 0
        all_raw_ocr_results = []  # ä¿å­˜æ‰€æœ‰åŸå§‹OCRç»“æœ

        for page_index, image_path in enumerate(image_files, 1):
            print(
                f"\nğŸ“„ [{page_index}/{len(image_files)}] æ­£åœ¨è¯†åˆ«: {os.path.basename(image_path)}"
            )
            page_start_time = time.time()

            ocr_result = self.recognize_paper(
                image_path=image_path,
                subject=subject,
                need_rotate=need_rotate,
                output_origin_points=output_origin_points,
            )
            
            # ä¿å­˜åŸå§‹OCRç»“æœ
            # æ³¨æ„: ocr_resultä¸­çš„Dataå­—æ®µå¯èƒ½æ˜¯JSONå­—ç¬¦ä¸²,éœ€è¦ååºåˆ—åŒ–
            ocr_result_to_save = ocr_result.copy()
            if isinstance(ocr_result_to_save.get("Data"), str):
                try:
                    ocr_result_to_save["Data"] = json.loads(ocr_result_to_save["Data"])
                except (json.JSONDecodeError, TypeError):
                    pass  # å¦‚æœè§£æå¤±è´¥,ä¿æŒåŸæ ·
            
            all_raw_ocr_results.append({
                "page": page_index,
                "image": os.path.basename(image_path),
                "ocr_result": ocr_result_to_save
            })
            
            parsed_result = self.parse_questions(ocr_result, page_index=page_index)

            # æå–å›¾ç‰‡
            if extract_images:
                page_output_dir = os.path.join(output_dir, f"page_{page_index}")
                extracted_count = self._extract_question_images(
                    parsed_result,
                    image_path,
                    page_output_dir,
                    min_figure_area=min_figure_area,
                    extract_figures=extract_figures,
                    extract_answer_areas=extract_answer_areas,
                )
                extracted_images_count += extracted_count
                if extracted_count > 0:
                    print(f"   ğŸ–¼ï¸  æå–äº† {extracted_count} å¼ å›¾ç‰‡")

            # ä¸ºæ¯ä¸ªéƒ¨åˆ†çš„é¢˜ç›®æ·»åŠ é¡µé¢ä¿¡æ¯
            page_question_count = 0
            for part in parsed_result.get("parts", []):
                for question in part.get("questions", []):
                    question["page"] = page_index
                    question["source_image"] = os.path.basename(image_path)
                    page_question_count += 1
                    total_questions_count += 1

            page_elapsed = time.time() - page_start_time
            print(f"   âœ“ è¯†åˆ«åˆ° {page_question_count} é“é¢˜ç›® (è€—æ—¶: {page_elapsed:.2f}ç§’)")

            # åˆå¹¶parts(å¦‚æœè·¨é¡µæœ‰ç›¸åŒçš„part_title,åˆ™åˆå¹¶)
            for part in parsed_result.get("parts", []):
                existing_part = next(
                    (p for p in all_parts if p["title"] == part["title"]), None
                )
                if existing_part:
                    existing_part["questions"].extend(part["questions"])
                else:
                    all_parts.append(part)

        total_elapsed = time.time() - total_start_time
        
        print(f"\n{'=' * 60}")
        print(f"âœ… è¯†åˆ«å®Œæˆ!")
        print(f"   æ€»é¢˜ç›®æ•°: {total_questions_count}")
        print(f"   æå–å›¾ç‰‡æ•°: {extracted_images_count}")
        print(f"   æ€»è€—æ—¶: {total_elapsed:.2f}ç§’")
        print(f"   å¹³å‡æ¯é¡µ: {total_elapsed / len(image_files):.2f}ç§’")
        print(f"{'=' * 60}")

        # æ„å»ºå¤„ç†åçš„ç»“æœ
        processed_result = {
            "metadata": {
                "exam_dir": os.path.abspath(exam_dir),
                "total_pages": len(image_files),
                "total_questions": total_questions_count,
                "extracted_images": extracted_images_count,
                "total_elapsed_time": round(total_elapsed, 2),
                "image_output_dir": os.path.abspath(output_dir) if extract_images else None,
            },
            "parts": all_parts,
        }
        
        # ä¿å­˜åŸå§‹OCRç»“æœåˆ°æ–‡ä»¶
        if save_raw_ocr:
            with open(save_raw_ocr, 'w', encoding='utf-8') as f:
                json.dump(all_raw_ocr_results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ åŸå§‹OCRç»“æœå·²ä¿å­˜åˆ°: {save_raw_ocr}")
        
        # ä¿å­˜å¤„ç†åçš„ç»“æœåˆ°æ–‡ä»¶
        if save_processed:
            with open(save_processed, 'w', encoding='utf-8') as f:
                json.dump(processed_result, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ å¤„ç†åç»“æœå·²ä¿å­˜åˆ°: {save_processed}")

        return processed_result

    def parse_questions(self, ocr_result: Dict, page_index: Optional[int] = None) -> Dict:
        """
        è§£æè¯†åˆ«ç»“æœ,æå–é¢˜ç›®ä¿¡æ¯

        Args:
            ocr_result: OCRè¯†åˆ«ç»“æœ

        Returns:
            æŒ‰éƒ¨åˆ†ç»„ç»‡çš„é¢˜ç›®ç»“æ„
        """
        parts = []

        # è·å–dataå­—æ®µ
        data = ocr_result.get("Data") or ocr_result.get("data")
        if not data:
            return {"parts": parts}

        # å¦‚æœdataæ˜¯å­—ç¬¦ä¸²,å°è¯•è§£æ
        if isinstance(data, str):
            try:
                data = json.loads(data)
            except:
                return {"parts": parts}

        # è·å–é¡¶å±‚çš„ figure æ•°ç»„(åŒ…å«æ‰€æœ‰å›¾ç‰‡ä¿¡æ¯)
        all_figures = data.get("figure", [])

        # è·å–part_info
        part_info = data.get("part_info", [])

        # éå†é¢˜å‹å¤§ç±»
        for part_index, part in enumerate(part_info, 1):
            part_title = part.get("part_title", "")
            questions = []

            # éå†è¯¥é¢˜å‹ä¸‹çš„æ‰€æœ‰é¢˜ç›®
            for q_index, subject in enumerate(part.get("subject_list", []), 1):
                question = self._parse_subject(
                    subject, part_index, q_index, part_title, all_figures, page_index
                )
                if question:
                    questions.append(question)

            if questions:
                parts.append(
                    {"title": part_title, "questions": questions}
                )

        return {"parts": parts}

    def _parse_subject(
        self,
        subject: Dict,
        part_index: int,
        q_index: int,
        part_title: str,
        all_figures: List[Dict],
        page_index: Optional[int],
    ) -> Optional[Dict]:
        """
        è§£æå•ä¸ªé¢˜ç›®

        Args:
            subject: é¢˜ç›®æ•°æ®
            part_index: éƒ¨åˆ†ç´¢å¼•
            q_index: é¢˜ç›®åœ¨éƒ¨åˆ†ä¸­çš„ç´¢å¼•
            part_title: é¢˜å‹åç§°
            all_figures: é¡¶å±‚çš„æ‰€æœ‰å›¾ç‰‡ä¿¡æ¯

        Returns:
            é¢˜ç›®ä¿¡æ¯å­—å…¸
        """
        # è·å–é¢˜ç›®ä½ç½®å¹¶è½¬æ¢ä¸ºè¾¹ç•Œæ¡†
        pos_list = self._normalize_pos_list(subject.get("pos_list", []))
        question_bbox = None
        if pos_list:
            question_bbox = self._pos_to_bbox(pos_list)

        # æ˜ å°„é¢˜ç›®ç±»å‹(æ ¹æ®é˜¿é‡Œäº‘å®˜æ–¹æ–‡æ¡£)
        # 0:é€‰æ‹©é¢˜ 1:å¡«ç©ºé¢˜ 2:é˜…è¯»ç†è§£ 3:å®Œå‹å¡«ç©º 4:é˜…è¯»å¡«ç©º 5:é—®ç­”é¢˜
        # 6:å¤šé€‰é¢˜ 7:å¡«ç©ºé€‰æ‹©æ··åˆ 8:åº”ç”¨é¢˜ 9:åˆ¤æ–­é¢˜ 10:ä½œå›¾é¢˜
        # 11:ææ–™é¢˜ 12:è®¡ç®—é¢˜ 13:è¿çº¿é¢˜ 14:ä½œæ–‡é¢˜ 15:è§£ç­”é¢˜ 16:å…¶ä»– 17:å›¾ 18:è¡¨æ ¼
        type_mapping = {
            0: {"id": "choice", "name": "é€‰æ‹©é¢˜"},
            1: {"id": "fill_blank", "name": "å¡«ç©ºé¢˜"},
            2: {"id": "reading_comprehension", "name": "é˜…è¯»ç†è§£"},
            3: {"id": "cloze", "name": "å®Œå‹å¡«ç©º"},
            4: {"id": "reading_fill", "name": "é˜…è¯»å¡«ç©º"},
            5: {"id": "answer", "name": "é—®ç­”é¢˜"},
            6: {"id": "multiple_choice", "name": "å¤šé€‰é¢˜"},
            7: {"id": "mixed", "name": "å¡«ç©ºé€‰æ‹©æ··åˆ"},
            8: {"id": "application", "name": "åº”ç”¨é¢˜"},
            9: {"id": "judge", "name": "åˆ¤æ–­é¢˜"},
            10: {"id": "drawing", "name": "ä½œå›¾é¢˜"},
            11: {"id": "material", "name": "ææ–™é¢˜"},
            12: {"id": "calculation", "name": "è®¡ç®—é¢˜"},
            13: {"id": "matching", "name": "è¿çº¿é¢˜"},
            14: {"id": "composition", "name": "ä½œæ–‡é¢˜"},
            15: {"id": "solution", "name": "è§£ç­”é¢˜"},
            16: {"id": "other", "name": "å…¶ä»–"},
            17: {"id": "figure", "name": "å›¾"},
            18: {"id": "table", "name": "è¡¨æ ¼"},
        }
        
        question_text = subject.get("text", "")
        raw_type = subject.get("type", 0)
        type_info = type_mapping.get(raw_type, {"id": "unknown", "name": "æœªçŸ¥é¢˜å‹"})
        question_type = type_info["id"]
        question_type_name = type_info["name"]
        
        question_id = f"{part_index}-{q_index}"
        if page_index is not None:
            question_id = f"{page_index}-{question_id}"
        question = {
            "id": question_id,
            "type": question_type,          # è‹±æ–‡æ ‡è¯†
            "type_name": question_type_name,# ä¸­æ–‡åç§°
            "raw_type": raw_type,           # é˜¿é‡Œäº‘åŸå§‹typeç±»å‹å€¼
            "prob": subject.get("prob", 1.0),                 # è¯†åˆ«ç½®ä¿¡åº¦
            "num_choices": subject.get("num_choices", 0),     # é€‰é¡¹æ•°é‡(å¦‚æœæœ‰)
            "table_list": subject.get("table_list", []),      # è¡¨æ ¼ä¿¡æ¯(å¦‚æœæœ‰)
            "text": question_text,
            "position": question_bbox,
            "options": [],
            "answer_areas": [],
            "figures": [],
        }

        stem_texts = []
        for element in subject.get("element_list", []):
            element_type = element.get("type", 0)
            content_list = element.get("content_list", [])

            option_items = []
            pending_marker = ""
            for content in content_list:
                content_string = content.get("string", "").strip()
                if not content_string:
                    continue
                marker, text = self._split_option_content(content_string)
                if marker:
                    if not text or text == marker:
                        pending_marker = marker
                        continue
                    option_items.append((marker, text, content.get("pos", [])))
                    pending_marker = ""
                else:
                    if pending_marker:
                        option_items.append((pending_marker, content_string, content.get("pos", [])))
                        pending_marker = ""

            # åˆ¤æ–­æ˜¯å¦ä¸ºé€‰é¡¹ï¼š
            # 1. æ˜ç¡®æ ‡è®°ä¸ºé€‰é¡¹ç±»å‹ (type=1)
            # 2. æˆ–è€…åœ¨é¢˜å¹²ä¸­æ‰¾åˆ°äº†è¶³å¤Ÿå¤šçš„é€‰é¡¹ç‰¹å¾ (é€šè¿‡ä¿®å¤æ­£åˆ™å, >=2 å·²ç»å¾ˆå®‰å…¨)
            is_option_element = element_type == 1 or len(option_items) >= 2

            if is_option_element and option_items:
                for marker, text, pos in option_items:
                    question["options"].append(
                        {
                            "option": marker,
                            "text": text,
                            "position": self._pos_to_bbox(pos) if pos else None,
                        }
                    )
            elif element_type == 0:
                # åªæœ‰éé€‰é¡¹å…ƒç´ çš„çº¯æ–‡æœ¬ï¼Œæ‰ä¼šè¢«åŠ å…¥åˆ°é¢˜å¹²ç»„åˆé‡Œ
                stem_texts.append(element.get("text", ""))

        if stem_texts:
            combined_text = "".join(stem_texts).strip()
            if combined_text:
                question["text"] = combined_text


        # è§£æç­”æ¡ˆåŒºåŸŸ(ç»Ÿä¸€ä½¿ç”¨è¾¹ç•Œæ¡†æ ¼å¼)
        for answer in subject.get("answer_list", []):
            if isinstance(answer, dict):
                pos = answer.get("pos", answer.get("pos_list", []))
            else:
                pos = answer
            pos_list = self._normalize_pos_list(pos)
            bbox = self._pos_to_bbox(pos_list) if pos_list else None
                
            if bbox:
                question["answer_areas"].append({"position": bbox})

        # è§£æå›¾ç‰‡(å»é‡å¹¶ç»Ÿä¸€æ ¼å¼)
        seen_figures = set()  # ç”¨äºå»é‡
        
        # ä» figure_list æå–
        for figure in subject.get("figure_list", []):
            if isinstance(figure, dict):
                pos = figure.get("pos", [])
            else:
                pos = figure

            pos_list = self._normalize_pos_list(pos)
            if pos_list:
                bbox = self._pos_to_bbox(pos_list)
                # ä½¿ç”¨ä½ç½®ä½œä¸ºå”¯ä¸€æ ‡è¯†å»é‡
                fig_key = (bbox.get("x"), bbox.get("y"), bbox.get("width"), bbox.get("height"))
                if fig_key not in seen_figures:
                    seen_figures.add(fig_key)
                    question["figures"].append({"position": bbox})

        # ä»é¡¶å±‚ figure æ•°ç»„ä¸­åŒ¹é…(åªæå–é…å›¾,æ’é™¤é¢˜ç›®æ•´ä½“æˆªå›¾)
        if question_bbox and all_figures:
            for figure in all_figures:
                fig_type = figure.get("type", "")
                if fig_type.startswith("subject_"):
                    fig_points = self._normalize_pos_list(figure.get("points", []))
                    if fig_points and self._is_figure_in_question(fig_points, question_bbox):
                        bbox = self._pos_to_bbox(fig_points)
                        fig_key = (bbox.get("x"), bbox.get("y"), bbox.get("width"), bbox.get("height"))
                        if fig_key not in seen_figures:
                            seen_figures.add(fig_key)
                            question["figures"].append({"position": bbox})

        return question

    def _is_figure_in_question(
        self, fig_points: List[Dict], question_bbox: Dict
    ) -> bool:
        """
        åˆ¤æ–­å›¾ç‰‡æ˜¯å¦åœ¨é¢˜ç›®èŒƒå›´å†…

        Args:
            fig_points: å›¾ç‰‡åæ ‡ç‚¹
            question_bbox: é¢˜ç›®è¾¹ç•Œæ¡†

        Returns:
            æ˜¯å¦åœ¨é¢˜ç›®èŒƒå›´å†…
        """
        fig_points = self._normalize_pos_list(fig_points)
        if not fig_points or not question_bbox:
            return False

        fig_bbox = self._pos_to_bbox(fig_points)

        # è®¡ç®—å›¾ç‰‡ä¸­å¿ƒç‚¹
        fig_center_x = fig_bbox["x"] + fig_bbox["width"] / 2
        fig_center_y = fig_bbox["y"] + fig_bbox["height"] / 2

        # åˆ¤æ–­ä¸­å¿ƒç‚¹æ˜¯å¦åœ¨é¢˜ç›®èŒƒå›´å†…
        return (
            question_bbox["x"]
            <= fig_center_x
            <= question_bbox["x"] + question_bbox["width"]
            and question_bbox["y"]
            <= fig_center_y
            <= question_bbox["y"] + question_bbox["height"]
        )

    def _split_option_content(self, text: str):
        if not text:
            return "", ""
        text = text.strip()
        markers = ["â‘ ","â‘¡","â‘¢","â‘£","â‘¤","â‘¥","â‘¦","â‘§","â‘¨","â‘©"]
        if text in markers:
            return text, ""
        patterns = [
            r"^([â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©])\s*(.*)$",
            r"^(\([A-Za-z]\))\s*(.*)$",
            r"^(\([0-9]{1,2}\))\s*(.*)$",
            r"^([A-Z])(?:[ã€\)]|\.(?!\d))\s*(.*)$",
            r"^([a-z])(?:[ã€\)]|\.(?!\d))\s*(.*)$",
            r"^([A-Z])\s+(.*)$",
            r"^([a-z])\s+(.*)$",
            r"^([0-9]{1,2})(?:[ã€\)]|\.(?!\d))\s*(.*)$",
        ]
        import re

        for pattern in patterns:
            match = re.match(pattern, text)
            if match:
                marker = match.group(1)
                option_text = match.group(2).strip() or text
                return marker, option_text
        return "", ""

    def _normalize_pos_list(self, pos) -> List[Dict]:
        if not pos:
            return []
        if isinstance(pos, dict):
            return [pos]
        if isinstance(pos, list):
            if pos and isinstance(pos[0], dict):
                return pos
            flattened = []
            for item in pos:
                if isinstance(item, dict):
                    flattened.append(item)
                elif isinstance(item, list):
                    flattened.extend(self._normalize_pos_list(item))
            return flattened
        return []

    def _pos_to_bbox(self, pos: List[Dict]) -> Dict:
        """
        å°†åæ ‡ç‚¹åˆ—è¡¨è½¬æ¢ä¸ºè¾¹ç•Œæ¡†

        Args:
            pos: åæ ‡ç‚¹åˆ—è¡¨ [{'x': x1, 'y': y1}, ...]

        Returns:
            è¾¹ç•Œæ¡† {'x': min_x, 'y': min_y, 'width': w, 'height': h}
        """
        pos = self._normalize_pos_list(pos)
        if not pos:
            return {}

        xs = [p["x"] for p in pos]
        ys = [p["y"] for p in pos]

        min_x, max_x = min(xs), max(xs)
        min_y, max_y = min(ys), max(ys)

        return {"x": min_x, "y": min_y, "width": max_x - min_x, "height": max_y - min_y}

    def _extract_question_images(
        self,
        parsed_result: Dict,
        source_image_path: str,
        output_dir: str,
        min_figure_area: int = 2000,
        extract_figures: bool = True,
        extract_answer_areas: bool = False,
    ) -> int:
        """
        æå–é¢˜ç›®ä¸­çš„å›¾ç‰‡

        Args:
            parsed_result: è§£æåçš„ç»“æœ(åŒ…å«partsç»“æ„)
            source_image_path: åŸå§‹å›¾ç‰‡è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            æå–çš„å›¾ç‰‡æ•°é‡
        """
        try:
            from PIL import Image
        except ImportError:
            print("âš ï¸  éœ€è¦å®‰è£… Pillow: pip install Pillow")
            return 0

        os.makedirs(output_dir, exist_ok=True)
        extracted_count = 0

        with Image.open(source_image_path) as img:
            # éå†æ‰€æœ‰éƒ¨åˆ†å’Œé¢˜ç›®
            for part in parsed_result.get("parts", []):
                for question in part.get("questions", []):
                    q_id = question["id"]  # ä½¿ç”¨æ–°çš„å…¨å±€å”¯ä¸€ID

                    if extract_figures:
                        for fig_idx, figure in enumerate(question.get("figures", []), 1):
                            bbox = figure.get("position")
                            area = 0
                            if bbox:
                                area = max(0, int(bbox.get("width", 0))) * max(
                                    0, int(bbox.get("height", 0))
                                )
                            if (
                                bbox
                                and bbox.get("width", 0) > 0
                                and bbox.get("height", 0) > 0
                                and area >= min_figure_area
                            ):
                                output_path = os.path.join(
                                    output_dir, f"q{q_id.replace('-', '_')}_figure_{fig_idx}.png"
                                )
                                self._crop_and_save(img, bbox, output_path)
                                figure["image_file"] = os.path.basename(output_path)
                                extracted_count += 1

                    if extract_answer_areas:
                        for area_idx, area in enumerate(question.get("answer_areas", []), 1):
                            bbox = area.get("position") if isinstance(area, dict) else None
                            if bbox and bbox.get("width", 0) > 0 and bbox.get("height", 0) > 0:
                                output_path = os.path.join(
                                    output_dir, f"q{q_id.replace('-', '_')}_answer_{area_idx}.png"
                                )
                                self._crop_and_save(img, bbox, output_path)
                                area["image_file"] = os.path.basename(output_path)
                                extracted_count += 1

        return extracted_count

    def _crop_and_save(self, img, bbox: Dict, output_path: str):
        """
        è£å‰ªå¹¶ä¿å­˜å›¾ç‰‡

        Args:
            img: PIL Image å¯¹è±¡
            bbox: è¾¹ç•Œæ¡†
            output_path: è¾“å‡ºè·¯å¾„
        """
        x = int(bbox["x"])
        y = int(bbox["y"])
        w = int(bbox["width"])
        h = int(bbox["height"])

        # PILçš„cropä½¿ç”¨(left, upper, right, lower)æ ¼å¼
        cropped = img.crop((x, y, x + w, y + h))
        cropped.save(output_path)

    def parse_baidu_questions(self, baidu_data: Dict) -> List[Dict]:
        questions = []
        qus_list = baidu_data.get("qus_result") or []
        for idx, qus in enumerate(qus_list, 1):
            pos_points = qus.get("qus_location", {}).get("points") or []
            bbox = self._pos_to_bbox(pos_points)
            q = {
                "id": f"b-{idx}",
                "type": qus.get("qus_type"),
                "text": "",
                "position": bbox,
                "options": [],
                "answer_areas": [],
            }
            elems = qus.get("qus_element") or []
            words = []
            for e in elems:
                e_words = e.get("elem_word") or []
                for w in e_words:
                    words.append(w)
                if e.get("elem_type") == "2":
                    e_pos = e.get("elem_location", {}).get("points") or []
                    eb = self._pos_to_bbox(e_pos)
                    if eb:
                        q["answer_areas"].append({"position": eb})
            full_text = []
            option_items = []
            pending_marker = ""
            for w in words:
                s = (w.get("word") or "").strip()
                if not s:
                    continue
                marker, text = self._split_option_content(s)
                wl = w.get("word_location") or {}
                wb = {
                    "x": int(wl.get("left", 0)),
                    "y": int(wl.get("top", 0)),
                    "width": int(wl.get("width", 0)),
                    "height": int(wl.get("height", 0)),
                }
                if marker:
                    if not text or text == marker:
                        pending_marker = marker
                        continue
                    option_items.append(
                        {"option": marker, "text": text, "position": wb}
                    )
                    pending_marker = ""
                else:
                    if pending_marker:
                        option_items.append(
                            {"option": pending_marker, "text": s, "position": wb}
                        )
                        pending_marker = ""
                    else:
                        full_text.append(s)
            q["text"] = " ".join(full_text).strip()
            q["options"] = option_items
            questions.append(q)
        return questions

    def merge_with_baidu(self, ali_parts: Dict, baidu_questions: List[Dict]) -> Dict:
        def center(b):
            return (b["x"] + b["width"] / 2, b["y"] + b["height"] / 2)
        def contains(b, cx, cy):
            return b and (b["x"] <= cx <= b["x"] + b["width"] and b["y"] <= cy <= b["y"] + b["height"])
        for part in ali_parts.get("parts", []):
            for q in part.get("questions", []):
                qb = q.get("position")
                if not qb:
                    continue
                cx, cy = center(qb)
                candidates = []
                for bq in baidu_questions:
                    bp = bq.get("position")
                    if bp and contains(bp, cx, cy):
                        candidates.append(bq)
                if not candidates:
                    continue
                bq = candidates[0]
                if not q.get("options"):
                    q["options"] = bq.get("options", [])
                else:
                    need_fill = any((not o.get("text") or o.get("text") == o.get("option")) for o in q["options"])
                    if need_fill and bq.get("options"):
                        q["options"] = bq["options"]
                if not q.get("text"):
                    q["text"] = bq.get("text", q.get("text", ""))
                if not q.get("answer_areas"):
                    q["answer_areas"] = bq.get("answer_areas", [])
        return ali_parts
    def _list_image_files(self, input_dir: str) -> List[str]:
        """
        åˆ—å‡ºç›®å½•ä¸­çš„æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶

        Args:
            input_dir: è¾“å…¥ç›®å½•

        Returns:
            å›¾ç‰‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        input_path = Path(input_dir)
        if not input_path.is_dir():
            raise ValueError(f"ç›®å½•ä¸å­˜åœ¨: {input_dir}")

        image_extensions = {".png", ".jpg", ".jpeg", ".bmp", ".gif", ".tiff", ".webp"}

        image_files = [
            str(path)
            for path in sorted(input_path.iterdir())
            if path.is_file() and path.suffix.lower() in image_extensions
        ]

        return image_files
